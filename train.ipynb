{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPhqiXMqgjh7EkFJyazH4FP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U80hgFtuPM61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, random\n",
        "\n",
        "from keras import backend as K\n",
        "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "def set_keras_backend(backend):\n",
        "\n",
        "    if K.backend() != backend:\n",
        "        os.environ['KERAS_BACKEND'] = backend\n",
        "        reload(K)\n",
        "        assert K.backend() == backend\n",
        "\n",
        "set_keras_backend(\"tensorflow\")\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "import h5py\n",
        "import numpy as np\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.layers import Input, merge, Lambda\n",
        "from keras.layers.core import Reshape, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, UpSampling2D, AveragePooling2D, \\\n",
        "    Conv2DTranspose\n",
        "from keras.layers.normalization import *\n",
        "from keras.optimizers import *\n",
        "from keras import initializers\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, random, sys, keras\n",
        "from keras.models import Model\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os, sys\n",
        "from functools import partial\n",
        "\n",
        "import random"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYwpE1XbGGU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f623a81c-4870-4018-f67d-77a354a75df8"
      },
      "source": [
        "##### load and preprocess the dataset ##\n",
        "batch_size = 256\n",
        "num_ep = 7 # number of facial expressions (referenced in y_train2)\n",
        "num_pp = 6 # number of identities (referenced in y_train1)\n",
        "epochs = 5000\n",
        "img_rows, img_cols = 64, 64\n",
        "c_dim = num_pp\n",
        "date = 2020\n",
        "\n",
        "# Extracting data from processed h5py file\n",
        "print ('Loading data...')\n",
        "f = h5py.File('/content/drive/My Drive/pprl_vgan/processed_dataset.h5')\n",
        "print ('Finished loading...')\n",
        "\n",
        "dataset_size = 26880\n",
        "training_size = 21504\n",
        "test_size = 5376\n",
        "\n",
        "# Extracting information from htf5 files\n",
        "x_train = f['x_train'][()][:]\n",
        "x_test  = f['x_test'][()][:]\n",
        "y_train1 = f['y_train1'][()][:]\n",
        "y_test1  = f['y_test1'][()][:]\n",
        "y_train2 = f['y_train2'][()][:]\n",
        "y_test2  = f['y_test2'][()][:]\n",
        "\n",
        "# Change data into correct form\n",
        "y_train1 = keras.utils.to_categorical(y_train1, num_pp)\n",
        "y_test1  = keras.utils.to_categorical(y_test1, num_pp)\n",
        "y_train2 = keras.utils.to_categorical(y_train2, num_ep)\n",
        "y_test2  = keras.utils.to_categorical(y_test2, num_ep)\n",
        "\n",
        "x_ori = np.divide(x_train, 255)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Finished loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzSkm_3XGJve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(ee):\n",
        "    ## save to numpyz###############\n",
        "    c = np.random.randint(num_pp, size=x_train.shape[0])\n",
        "    c_train = keras.utils.to_categorical(c, num_pp)\n",
        "    c = np.random.randint(num_pp, size=x_test.shape[0])\n",
        "    c_test = keras.utils.to_categorical(c, num_pp)\n",
        "\n",
        "    [z_train, mean_var_train] = encoder.predict(x_train)\n",
        "    encoded_xtrain = decoder.predict([z_train, c_train])\n",
        "\n",
        "    [z_test, mean_var_test] = encoder.predict(x_test)\n",
        "    encoded_xtest = decoder.predict([z_test, c_test])\n",
        "\n",
        "    np.savez('/Z_' + str(date) + 'epoch'+str(ee)+'_64_64_VAE_GAN_labelfull_v2.npz',\n",
        "             encoded_xtrain, y_train1, y_train2, c_train, encoded_xtest, y_test1, y_test2, c_test)\n",
        "    np.savez('/X_' + str(date) + 'epoch'+str(ee)+ '_fi_512_VAE_GAN_labelfull_v2.npz',\n",
        "             z_train, y_train1, y_train2, c_train, z_test, y_test1, y_test2, c_test)\n",
        "\n",
        "opt  = RMSprop(lr=0.0002, decay=1e-6)\n",
        "dopt = RMSprop(lr=0.0002, decay=1e-6)\n",
        "\n",
        "# Define KL loss function for network training\n",
        "def KL_loss(y_true, y_pred):\n",
        "    z_mean = y_pred[:, 0:z_dim]\n",
        "    z_log_var = y_pred[:, z_dim:2 * z_dim]\n",
        "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    return K.mean(kl_loss)\n",
        "\n",
        "# Returns sample from probability distribution (uses Keras backend)\n",
        "epsilon_std = 1\n",
        "def sampling(args):\n",
        "    z_mean, z_log_sigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], z_dim), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    return (z_mean + K.exp(K.square(z_log_sigma) / 2) * epsilon)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7PTsB_zG69x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Defining Encoder and Decoder\n",
        "\n",
        "# This method takes in the input z_dim (shape of latent space), input shape, units and dropout\n",
        "# Then, it returns a keras.Model which takes in an 'Input' image x with shape=input_shape\n",
        "# and which outputs the encoded shape z, and the probability distribution h2\n",
        "def model_encoder(z_dim, input_shape, units=512, dropout=0.3):\n",
        "    k = 8\n",
        "    x = Input(input_shape)\n",
        "    h = Conv2D(int(units/8), (k, k), strides=(2, 2), padding='same')(x)\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "    h = Dropout(dropout)(h)\n",
        "    h = LeakyReLU(0.2)(h)\n",
        "    h = Conv2D(int(units/4), (k, k), strides=(2, 2), padding='same')(h)\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "    h = Dropout(dropout)(h)\n",
        "    h = LeakyReLU(0.2)(h)\n",
        "    h = Conv2D(int(units/2), (k, k), strides=(2, 2), padding='same')(h)\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "    h = Dropout(dropout)(h)\n",
        "    h = LeakyReLU(0.2)(h)\n",
        "    h = Conv2D(units, (k, k), strides=(2, 2), padding='same')(h)\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "    h = Dropout(dropout)(h)\n",
        "    h = LeakyReLU(0.2)(h)\n",
        "    h = Flatten()(h)\n",
        "    mean = Dense(z_dim, name=\"encoder_mean\")(h)\n",
        "    logvar = Dense(z_dim, name=\"encoder_sigma\", activation='sigmoid')(h)\n",
        "\n",
        "    z = Lambda(function=sampling, output_shape=(z_dim,))([mean, logvar])\n",
        "    h2 = keras.layers.concatenate([mean, logvar])\n",
        "    return [Model(x, [z, h2], name='Encoder'), mean, logvar]\n",
        "\n",
        "\n",
        "# This function takes z_dim and c_dim as inputs, where z_dim is the shape of the latent space, and c_dim is\n",
        "# the number of people (6 in this case), in binary form. It returns a keras.Model which takes the encoded\n",
        "# image 'x', and identity information 'auxiliary_c' as input, and returns a decoded image h\n",
        "def model_decoder(z_dim, c_dim):\n",
        "    k = 8\n",
        "    x = Input(shape=(z_dim,))\n",
        "    auxiliary_c = Input(shape=(c_dim,), name='aux_input_c')\n",
        "    h = keras.layers.concatenate([x, auxiliary_c])\n",
        "    h = Dense(4 * 4 * 128, activation='relu')(h)\n",
        "    h = Reshape((4, 4, 128))(h)\n",
        "    h = Conv2DTranspose(units, (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 32*32*64\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "    h = Conv2DTranspose(int(units/2), (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 64*64*64\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "    h = Conv2DTranspose(int(units/2), (k, k), strides=(2, 2), padding='same', activation='relu')(h)  # 8*6*64\n",
        "    h = BatchNormalization(momentum=0.8)(h)\n",
        "\n",
        "    h = Conv2DTranspose(3, (k, k), strides=(2, 2), padding='same', activation='sigmoid')(h)  # 8*6*64\n",
        "    return Model([x, auxiliary_c], h, name=\"Decoder\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prWUoYNhG9kd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15576ece-3882-4649-8f1b-ef47bd200eb5"
      },
      "source": [
        "def KL_reconstruction_loss(y_true, y_pred):\n",
        "    reconstruction_loss = keras.losses.binary_crossentropy(K.flatten(y_true), K.flatten(y_pred))\n",
        "    return K.mean(0.5*keras.losses.kullback_leibler_divergence(y_true, y_pred) + reconstruction_loss)\n",
        "\n",
        "# #### Build GAN model ####\n",
        "z_dim = 128\n",
        "units = 256\n",
        "GANloss_weights_vae = Input(shape = (1,))\n",
        "GANtargets_vae  = Input(shape = (z_dim*2,))\n",
        "\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "#ee = 100\n",
        "x = Input(input_shape)\n",
        "auxiliary_c = Input(shape=(c_dim,), name='aux_input_c')\n",
        "\n",
        "#[encoder, z_mean, z_logvar] = model_encoder(z_dim=z_dim, input_shape=(img_rows, img_cols, 3), units=units, dropout=0.3)\n",
        "encoder = model_encoder(z_dim=z_dim, input_shape=(img_rows, img_cols, 3), units=units, dropout=0.3)[0]\n",
        "z_mean = model_encoder(z_dim=z_dim, input_shape=(img_rows, img_cols, 3), units=units, dropout=0.3)[1]\n",
        "z_logvar = model_encoder(z_dim=z_dim, input_shape=(img_rows, img_cols, 3), units=units, dropout=0.3)[2]\n",
        "encoder.load_weights('/content/drive/My Drive/pprl_vgan/trained_weights_01/encoder_2020epochs2000.h5')\n",
        "encoder.compile(loss='binary_crossentropy', optimizer=opt, experimental_run_tf_function=False)\n",
        "encoder.summary()\n",
        "\n",
        "decoder = model_decoder(z_dim=z_dim, c_dim=c_dim)\n",
        "decoder.load_weights('/content/drive/My Drive/pprl_vgan/trained_weights_01/decoder_2020epochs2000.h5')\n",
        "decoder.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "decoder.summary()\n",
        "\n",
        "vae = Model([x, auxiliary_c], decoder([encoder(x)[0], auxiliary_c]), name='VAE')\n",
        "vae.compile(loss=KL_reconstruction_loss, optimizer=opt)\n",
        "vae.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 32)   6176        input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 32, 32, 32)   128         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 32, 32, 32)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)      (None, 32, 32, 32)   0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 64)   131136      leaky_re_lu_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 64)   256         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 16, 16, 64)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)      (None, 16, 16, 64)   0           dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 128)    524416      leaky_re_lu_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 8, 8, 128)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)      (None, 8, 8, 128)    0           dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 4, 4, 256)    2097408     leaky_re_lu_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 256)    1024        conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 256)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_54 (LeakyReLU)      (None, 4, 4, 256)    0           dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 4096)         0           leaky_re_lu_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_mean (Dense)            (None, 128)          524416      flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder_sigma (Dense)           (None, 128)          524416      flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 128)          0           encoder_mean[0][0]               \n",
            "                                                                 encoder_sigma[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 256)          0           encoder_mean[0][0]               \n",
            "                                                                 encoder_sigma[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 3,809,888\n",
            "Trainable params: 3,808,928\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"Decoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_34 (InputLayer)           [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "aux_input_c (InputLayer)        [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 134)          0           input_34[0][0]                   \n",
            "                                                                 aux_input_c[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2048)         276480      concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 4, 4, 128)    0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 8, 8, 256)    2097408     reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 256)    1024        conv2d_transpose_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 16, 16, 128)  2097280     batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 16, 16, 128)  512         conv2d_transpose_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 32, 32, 128)  1048704     batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 128)  512         conv2d_transpose_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DTran (None, 64, 64, 3)    24579       batch_normalization_59[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 5,546,499\n",
            "Trainable params: 5,545,475\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"VAE\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_30 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder (Functional)            [(None, 128), (None, 3809888     input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "aux_input_c (InputLayer)        [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder (Functional)            (None, 64, 64, 3)    5546499     Encoder[0][0]                    \n",
            "                                                                 aux_input_c[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 9,356,387\n",
            "Trainable params: 9,354,403\n",
            "Non-trainable params: 1,984\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKA9VCh0HAgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotGeneratedImages(epoch, idx=0, examples=10, dim=(10, 10), figsize=(10, 10)):\n",
        "    n = num_pp*2  # how many digits we will display\n",
        "    pp_avg = 4500\n",
        "    plt.figure(figsize=(16, 4))\n",
        "\n",
        "    sample = x_ori[idx:idx + n, :, :, :]\n",
        "    c = np.asarray([0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0]) #output identities\n",
        "    c = keras.utils.to_categorical(c, num_pp)\n",
        "\n",
        "    [z, mean_var] = encoder.predict(sample)\n",
        "    generated_images = decoder.predict([z, c])\n",
        "\n",
        "    for i in range(n):\n",
        "        # display original\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        ori = sample[i].reshape(img_rows, img_cols, 3)\n",
        "        ori = np.uint8(ori * 255)\n",
        "        plt.imshow(ori)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        # display reconstruction\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        rec = generated_images[i].reshape(img_rows, img_cols, 3)\n",
        "        rec = np.uint8(rec * 255)\n",
        "        plt.imshow(rec)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Path to be created\n",
        "    #plt.savefig(path + '/GAN_MUG_results_' + str(date) + '_generated_image_epoch_%d.tif' % epoch)\n",
        "    #plt.savefig()\n",
        "    #plt.close()\n",
        "    \n",
        "    plt.savefig('GAN_MUG_results_' + str(date) + '_generated_image_epoch_%d.png' % epoch)\n",
        "    #f = h5py.File('GAN_MUG_results_' + str(date) + '_generated_image_epoch_%d.tif' % epoch, 'w')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAiw5YOtHEkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator_for_n(nb_epoch=1, plt_frq=25, BATCH_SIZE=256):\n",
        "    # This function warms up the generator by training it against the original dataset.\n",
        "    batchCount = int(x_ori.shape[0] / BATCH_SIZE)\n",
        "    for ee in range(1, nb_epoch+1):\n",
        "        print('-' * 15, 'Warm-up Epoch %d' % ee, '-' * 15)\n",
        "        for e in tqdm(range(batchCount)):\n",
        "            idx = random.sample(range(0, x_ori.shape[0]), BATCH_SIZE)\n",
        "            image_batch = x_ori[idx, :, :, :]\n",
        "            c = y_train1[idx, :]\n",
        "            \n",
        "            [z, mean_var] = encoder.predict(image_batch)\n",
        "            y1_batch = y_train1[idx, :]\n",
        "            y2_batch = y_train2[idx, :]\n",
        "            \n",
        "            y0_dist_real = np.random.uniform(0.9, 1.0, size=[BATCH_SIZE, 1])\n",
        "            y0_dist_fake = np.random.uniform(0, 0.1, size=[BATCH_SIZE, 1])\n",
        "            \n",
        "            vae_loss = vae.fit([image_batch, c], image_batch, batch_size=32)\n",
        "            \n",
        "            #g_loss = GAN.train_on_batch(image_batch, image_batch)\n",
        "            #if ee % 1 == 0:\n",
        "                #vae.save('vae_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "                #encoder.save('encoder_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "                #decoder.save('decoder_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "\n",
        "            #GAN.save('VAEGAN_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "            #if ee % 1 == 0:\n",
        "            #plotGeneratedImages(epoch=ee + 40, idx=75)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI_q0HOmHGkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################ Build the discrminator ###########################################################################\n",
        "\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "# \n",
        "loss_weights_1= Input(shape=(1,), name='disc_1')\n",
        "loss_weights_2= Input(shape=(1,),name='disc_2')\n",
        "loss_weights_3= Input(shape=(1,),name='disc_3')\n",
        "\n",
        "#\n",
        "targets1  = Input(shape = (1,),name='disc_4')\n",
        "targets2  = Input(shape = (num_pp,),name='disc_5')\n",
        "targets3  = Input(shape = (num_ep,),name='disc_6')\n",
        "\n",
        "#\n",
        "d_input   = Input(input_shape,name='disc_7')\n",
        "rep_field = 8\n",
        "x = Conv2D(32, (rep_field, rep_field), strides=(2, 2), padding='same', name='id_conv1')(d_input)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(64, (rep_field, rep_field), strides=(2, 2), padding='same', name='id_conv2')(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(128, (rep_field, rep_field), strides=(2, 2), padding='same', name='id_conv3')(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Conv2D(256, (rep_field, rep_field), strides=(2, 2), padding='same', name='id_conv4')(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, name='ds')(x)\n",
        "x = LeakyReLU(0.2)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_binary     = Dense(1, activation='sigmoid', name='bin_real')(x)\n",
        "output_identity   = Dense(num_pp, activation='softmax', name='id_real')(x)\n",
        "output_expression = Dense(num_ep, activation='softmax', name='exp_real')(x)\n",
        "\n",
        "discriminator = Model([d_input, loss_weights_1, loss_weights_2,loss_weights_3, targets1, targets2, targets3], [output_binary, output_identity, output_expression])\n",
        "\n",
        "from keras import losses\n",
        "\n",
        "loss =loss_weights_1*losses.binary_crossentropy(targets1,output_binary) + \\\n",
        "      loss_weights_2*losses.categorical_crossentropy(targets2,output_identity)+ \\\n",
        "      loss_weights_3*losses.categorical_crossentropy(targets3,output_expression)\n",
        "discriminator.add_loss(loss)\n",
        "discriminator.compile( optimizer=dopt, loss = None)\n",
        "\n",
        "discriminator = tf.keras.models.load_model('/content/drive/My Drive/pprl_vgan/trained_weights_01/discriminator_2020epochs2000.h5')\n",
        "discriminator.summary()\n",
        "print (discriminator.metrics_names)\n",
        "#plot_model(discriminator, to_file = '/media/vivo/New Volume/FERG_DB_256/stats/disc_0605_model.png')\n",
        "\n",
        "\n",
        "def make_trainable(net, val):\n",
        "    net.trainable = val\n",
        "    for l in net.layers:\n",
        "        l.trainable = val\n",
        "\n",
        "\n",
        "make_trainable(discriminator, False)\n",
        "discriminator.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1lqJBY0HIqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminator_for_n(nb_epoch=50000, plt_frq=25, BATCH_SIZE=256):\n",
        "    batchCount = int(x_ori.shape[0] / BATCH_SIZE)\n",
        "    for ee in range(1, nb_epoch+1):\n",
        "        print('-' * 15, 'Warm-up Epoch %d' % ee, '-' * 15)\n",
        "        for e in tqdm(range(batchCount)):\n",
        "            idx = random.sample(range(0, x_ori.shape[0]), BATCH_SIZE)\n",
        "            image_batch = x_ori[idx, :, :, :]\n",
        "\n",
        "            y1_batch = y_train1[idx, :]\n",
        "            y2_batch = y_train2[idx, :]\n",
        "            \n",
        "            y0_dist_real = np.random.uniform(0.9, 1.0, size=[BATCH_SIZE, 1])\n",
        "            y0_dist_fake = np.random.uniform(0, 0.1, size=[BATCH_SIZE, 1])\n",
        "\n",
        "            make_trainable(discriminator, True)\n",
        "            discriminator.trainable = True\n",
        "            loss_weights_1 = np.ones(shape = (batch_size,))*1/4.0\n",
        "            loss_weights_2 = np.ones(shape = (batch_size,))*1/2.0\n",
        "            loss_weights_3 = np.ones(shape = (batch_size,))*1/4.0\n",
        "            # Train discriminator on real image\n",
        "            d_loss_real = discriminator.train_on_batch([image_batch, loss_weights_1, loss_weights_2, loss_weights_3,y0_dist_real, y1_batch, y2_batch],y= None)\n",
        "            loss_weights_1 = np.ones(shape=(batch_size,))*1.0\n",
        "            loss_weights_2 = np.ones(shape=(batch_size,)) * 0\n",
        "            loss_weights_3 = np.ones(shape=(batch_size,)) * 0\n",
        "\n",
        "            make_trainable(discriminator, False)\n",
        "            discriminator.trainable = False\n",
        "\n",
        "            print(np.sum(d_loss_real))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgvdpwttHPJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #### Build GAN model ####\n",
        "z_dim = 128\n",
        "units = 256\n",
        "GANloss_weights_vae = Input(shape = (1,))\n",
        "GANtargets_vae  = Input(shape = (z_dim*2,))\n",
        "\n",
        "#ee = 100\n",
        "\n",
        "auxiliary_c = Input(shape=(c_dim,), name='aux_input_c')\n",
        "### Generate Image set ###\n",
        "# generate_dataset(ee=ee)\n",
        "###\n",
        "\n",
        "### GAN formulation ###\n",
        "[z, mean_var] = encoder(d_input)\n",
        "xpred = decoder([z, auxiliary_c])\n",
        "output_binary, output_identity, output_expression = discriminator([xpred, loss_weights_1, loss_weights_2,loss_weights_3, targets1, targets2, targets3])\n",
        "GAN = Model([d_input, auxiliary_c, GANloss_weights_vae, loss_weights_1,loss_weights_2,loss_weights_3, GANtargets_vae, targets1, targets2, targets3],\\\n",
        "            [mean_var, output_binary, output_identity, output_expression])\n",
        "\n",
        "GANloss = GANloss_weights_vae*KL_loss(GANtargets_vae, mean_var) + \\\n",
        "          loss_weights_1*losses.binary_crossentropy(targets1,output_binary) + \\\n",
        "          loss_weights_2*losses.categorical_crossentropy(targets2, output_identity)+ \\\n",
        "          loss_weights_3*losses.categorical_crossentropy(targets3, output_expression)\n",
        "GAN.add_loss(GANloss)\n",
        "#GAN.load_weights(GAN_path)\n",
        "GAN.compile(optimizer = opt, loss = None)\n",
        "GAN.summary()\n",
        "print (GAN.metrics_names)\n",
        "\n",
        "\n",
        "# plot_model(GAN, to_file = 'GAN_model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUHaaQhyHP_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_losses = []\n",
        "dis_losses = []\n",
        "\n",
        "# Check discriminator accuracy\n",
        "def discriminator_acc():\n",
        "    size = len(x_ori)\n",
        "\n",
        "    idx = random.sample(range(0, x_ori.shape[0]), size)\n",
        "    image_batch = x_ori[idx, :, :, :]\n",
        "\n",
        "    y1_batch = y_train1[idx, :]\n",
        "    y2_batch = y_train2[idx, :]\n",
        "    y0_batch = np.ones((size, 1))\n",
        "\n",
        "    loss_weights_1 = np.ones(shape = (size,))*1/4.0\n",
        "    loss_weights_2 = np.ones(shape = (size,))*1/2.0\n",
        "    loss_weights_3 = np.ones(shape = (size,))*1/4.0\n",
        "\n",
        "    [out_binary, out_identity, out_expression] = discriminator.predict([image_batch, loss_weights_1, loss_weights_2, loss_weights_3,y0_batch, y1_batch, y2_batch])\n",
        "    binary_acc = 0\n",
        "    identity_acc = 0\n",
        "    expression_acc = 0\n",
        "    \n",
        "    out_binary=np.round(out_binary,0)\n",
        "    #out_identity=np.round(out_identity,0)\n",
        "    #out_expression=np.round(out_expression,0)\n",
        "    \n",
        "    for i in range(size):\n",
        "        if (y0_batch[i]!=out_binary[i]):\n",
        "            binary_acc+=1\n",
        "        \n",
        "        ii = np.where(out_identity[i] == max(out_identity[i]))\n",
        "        #print(ii)\n",
        "        for j in range(len(out_identity[i])):\n",
        "            out_identity[i,j]=0\n",
        "        out_identity[i,ii]=1\n",
        "        comp = y1_batch[i]==out_identity[i]\n",
        "        if (not comp.all()):\n",
        "            identity_acc+=1\n",
        "        \n",
        "        ii = np.where(out_expression[i] == max(out_expression[i]))\n",
        "        #print(ii)\n",
        "        for j in range(len(out_expression[i])):\n",
        "            out_expression[i,j]=0\n",
        "        out_expression[i,ii] = 1\n",
        "        comp = y2_batch[i]==out_expression[i]\n",
        "        if (not comp.all()):\n",
        "            expression_acc+=1\n",
        "            \n",
        "        #print(y2_batch[i])\n",
        "        #print(out_expression[i])\n",
        "            \n",
        "    binary_acc = 1-(binary_acc/size)\n",
        "    identity_acc = 1-(identity_acc/size)\n",
        "    expression_acc = 1-(expression_acc/size)\n",
        "    \n",
        "    return([binary_acc, identity_acc, expression_acc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81y8dStzHYmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_for_n(nb_epoch=50000, plt_frq=25, BATCH_SIZE=256):\n",
        "    batchCount = int(x_ori.shape[0] / BATCH_SIZE)\n",
        "    for ee in range(1, nb_epoch + 1):\n",
        "        print('-' * 15, 'Epoch %d' % ee, '-' * 15)\n",
        "        #plotGeneratedImages(epoch=ee + 40, idx=75)\n",
        "        # val_bin_acc, val_id_acc, val_ep_acc = val_test()\n",
        "        for e in tqdm(range(batchCount)):\n",
        "            # for didx in xrange(0,k):\n",
        "            \n",
        "            # Selecting BATCH_SIZE number of random sample from x_train\n",
        "            idx = random.sample(range(0, x_ori.shape[0]),\n",
        "                                BATCH_SIZE)  # train discriminator twice more than the generator\n",
        "            image_batch = x_ori[idx, :, :, :]  # real data\n",
        "            # Create random 'identity'?\n",
        "            c = np.random.randint(num_pp, size=BATCH_SIZE)\n",
        "            c = keras.utils.to_categorical(c, num_pp)\n",
        "            # The batch is fed into encoder, and the output is then put into [z, mean_var]\n",
        "            # Then, the output z and random identity c is fed into decoder, and generated image is output\n",
        "            \n",
        "            [z, mean_var] = encoder.predict(image_batch)\n",
        "            generated_images = decoder.predict([z, c])\n",
        "            #generated_images = vae.predict([image_batch, c])\n",
        "            # Process the corresponding real identity and expression values\n",
        "            y1_batch = y_train1[idx, :]\n",
        "            y2_batch = y_train2[idx, :]\n",
        "\n",
        "            # generated_images = generator.predict([image_batch, c_, z])\n",
        "            y0_dist_real = np.random.uniform(0.9, 1.0, size=[BATCH_SIZE, 1])\n",
        "            y0_dist_fake = np.random.uniform(0, 0.1, size=[BATCH_SIZE, 1])\n",
        "\n",
        "            make_trainable(discriminator, True)\n",
        "            discriminator.trainable = True\n",
        "            loss_weights_1 = np.ones(shape = (batch_size,))*1/4.0\n",
        "            loss_weights_2 = np.ones(shape = (batch_size,))*1/2.0 # incentivize recognizing identity\n",
        "            loss_weights_3 = np.ones(shape = (batch_size,))*1/4.0\n",
        "            # Train discriminator on real image\n",
        "            d_loss_real = discriminator.train_on_batch([image_batch, loss_weights_1, loss_weights_2, loss_weights_3,y0_dist_real, y1_batch, y2_batch],y= None)\n",
        "            loss_weights_1 = np.ones(shape=(batch_size,))*1.0/10.0\n",
        "            loss_weights_2 = np.ones(shape=(batch_size,))*  0 # do not train identity and expression recognition against fake images...!\n",
        "            loss_weights_3 = np.ones(shape=(batch_size,))*  0\n",
        "            # Train discriminator on fake image\n",
        "            d_loss_fake = discriminator.train_on_batch([generated_images,loss_weights_1,loss_weights_2,loss_weights_3, y0_dist_fake, c, y2_batch], y = None)\n",
        "            \n",
        "\n",
        "\n",
        "            make_trainable(discriminator, False)\n",
        "            discriminator.trainable = False\n",
        "            for ii in range(0, 2):\n",
        "                idx = random.sample(range(0, x_ori.shape[0]),\n",
        "                                    BATCH_SIZE)  # train discriminator twice more than the generator\n",
        "                image_batch = x_ori[idx, :, :, :]  # real data\n",
        "                c = np.random.randint(num_pp, size=BATCH_SIZE)\n",
        "                c = keras.utils.to_categorical(c, num_pp)\n",
        "\n",
        "                mean_var_ref = np.ones((BATCH_SIZE, z_dim * 2))\n",
        "                y1_batch = y_train1[idx, :]\n",
        "                y2_batch = y_train2[idx, :]\n",
        "\n",
        "                y0_batch = np.ones((BATCH_SIZE, 1)) #0.002, 0.09, 0.8, 0.108\n",
        "                GANloss_weights_vae = np.ones(shape = (batch_size,))*0.002\n",
        "                loss_weights_1 = np.ones(shape = (batch_size,))*0.078 #0.078\n",
        "                loss_weights_2 = np.ones(shape = (batch_size,))*0.8 #0.8\n",
        "                loss_weights_3 = np.ones(shape = (batch_size,))*0.12 #0.12\n",
        "                g_loss = GAN.train_on_batch([image_batch, c, GANloss_weights_vae, loss_weights_1, loss_weights_2, loss_weights_3, mean_var_ref, y0_batch, c, y2_batch], y = None)\n",
        "                \n",
        "                \n",
        "        if ee % 2000 == 0:\n",
        "            GAN.save('VAEGAN_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "            encoder.save('encoder_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "            decoder.save('decoder_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "            vae.save('vae_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "            discriminator.save('discriminator_' + str(date) + 'epochs' + str(ee) + '.h5')\n",
        "        if ee % 50 == 0:\n",
        "            plotGeneratedImages(epoch=ee + 40, idx=75)\n",
        "            dis_losses.append(d_loss_fake)\n",
        "            gan_losses.append(np.sum(g_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IpDOc_VHbOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "train_for_n(nb_epoch=2000, plt_frq=500, BATCH_SIZE=batch_size)\n",
        "plotGeneratedImages(epoch=0, idx=75)\n",
        "\n",
        "print(discriminator_acc())\n",
        "\n",
        "process_time = time.time() - start_time\n",
        "print(\"Elapsed: %s \" % (process_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gBl_qufHefd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(gan_losses)\n",
        "plt.show()\n",
        "#print(gan_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvll_7s-HeoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_dis_losses = []\n",
        "for i in dis_losses:\n",
        "    new_dis_losses.append(np.sum(i))\n",
        "plt.plot(new_dis_losses)\n",
        "plt.show()\n",
        "print(new_dis_losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}